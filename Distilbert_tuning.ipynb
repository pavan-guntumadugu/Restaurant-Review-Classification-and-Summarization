{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289831ae-95ff-4f00-bc9f-6855801f5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f733ce-7f94-435b-a1d2-fb8782c758f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon-food_reviews.csv\")\n",
    "\n",
    "# Apply binary classification to 'Score'\n",
    "df['Score'] = df['Score'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "balanced_df = pd.DataFrame()\n",
    "positive_df = df[df['Score'] == 1].sample(n=5391, random_state=42)  # Downsample positive class\n",
    "negative_df = df[df['Score'] == 0].sample(n=5391, random_state=42)  # Assume we can take exactly 18391\n",
    "balanced_df = pd.concat([positive_df, negative_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026ce7bf-8ecc-43bb-a845-e21768d9de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gpava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = word_tokenize(text)  # Tokenize\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(text)\n",
    "\n",
    "balanced_df['processed_text'] = balanced_df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85edb3a8-cd5b-41be-9da7-9d01f342abe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Worst Hot Chocolate I've Ever Had!</td>\n",
       "      <td>I couldn't even taste the chocolate. It tasted...</td>\n",
       "      <td>could n't even taste chocolate . tasted like d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome Tea</td>\n",
       "      <td>I drank this tea at least 4 days a week during...</td>\n",
       "      <td>drank tea least 4 days week last 6 weeks pregn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>WONDERFUL You wont believe!</td>\n",
       "      <td>This brownie mix ROCKS. YOu wont believe how l...</td>\n",
       "      <td>brownie mix rocks . wont believe low cal ! who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Just didn't care for it.</td>\n",
       "      <td>I ordered a sample pack that had five of these...</td>\n",
       "      <td>ordered sample pack five . thank goodness . wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Very good, but a little overboard on the salt</td>\n",
       "      <td>I have a Pasadena popcorn machine from The Gre...</td>\n",
       "      <td>pasadena popcorn machine great northern popcor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10777</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed: Sad or displeased because someon...</td>\n",
       "      <td>For years, I used the generic store brand cann...</td>\n",
       "      <td>years , used generic store brand canned tomato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>1</td>\n",
       "      <td>Love the Taco Bell Sauce!</td>\n",
       "      <td>Happy that I received the Taco Bell Bold &amp; Cre...</td>\n",
       "      <td>happy received taco bell bold &amp; creamy spicy r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>1</td>\n",
       "      <td>Transportable y buen Sonido!</td>\n",
       "      <td>Muy buen producto. por su tama&amp;ntilde;o, uno l...</td>\n",
       "      <td>muy buen producto . por su tama &amp; ntilde ; , u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>1</td>\n",
       "      <td>1 year olds LOVE this flavor</td>\n",
       "      <td>This is the one flavor that my 1 year olds wil...</td>\n",
       "      <td>one flavor 1 year olds always eat - matter foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>0</td>\n",
       "      <td>Coffee was old and in a big clump</td>\n",
       "      <td>This coffee arrived fast but when I went to sh...</td>\n",
       "      <td>coffee arrived fast went shake cup big hard ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10782 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score                                            Summary  \\\n",
       "0          0                 Worst Hot Chocolate I've Ever Had!   \n",
       "1          1                                        Awesome Tea   \n",
       "2          1                        WONDERFUL You wont believe!   \n",
       "3          0                           Just didn't care for it.   \n",
       "4          1      Very good, but a little overboard on the salt   \n",
       "...      ...                                                ...   \n",
       "10777      0  Disappointed: Sad or displeased because someon...   \n",
       "10778      1                          Love the Taco Bell Sauce!   \n",
       "10779      1                       Transportable y buen Sonido!   \n",
       "10780      1                       1 year olds LOVE this flavor   \n",
       "10781      0                  Coffee was old and in a big clump   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      I couldn't even taste the chocolate. It tasted...   \n",
       "1      I drank this tea at least 4 days a week during...   \n",
       "2      This brownie mix ROCKS. YOu wont believe how l...   \n",
       "3      I ordered a sample pack that had five of these...   \n",
       "4      I have a Pasadena popcorn machine from The Gre...   \n",
       "...                                                  ...   \n",
       "10777  For years, I used the generic store brand cann...   \n",
       "10778  Happy that I received the Taco Bell Bold & Cre...   \n",
       "10779  Muy buen producto. por su tama&ntilde;o, uno l...   \n",
       "10780  This is the one flavor that my 1 year olds wil...   \n",
       "10781  This coffee arrived fast but when I went to sh...   \n",
       "\n",
       "                                          processed_text  \n",
       "0      could n't even taste chocolate . tasted like d...  \n",
       "1      drank tea least 4 days week last 6 weeks pregn...  \n",
       "2      brownie mix rocks . wont believe low cal ! who...  \n",
       "3      ordered sample pack five . thank goodness . wo...  \n",
       "4      pasadena popcorn machine great northern popcor...  \n",
       "...                                                  ...  \n",
       "10777  years , used generic store brand canned tomato...  \n",
       "10778  happy received taco bell bold & creamy spicy r...  \n",
       "10779  muy buen producto . por su tama & ntilde ; , u...  \n",
       "10780  one flavor 1 year olds always eat - matter foo...  \n",
       "10781  coffee arrived fast went shake cup big hard ba...  \n",
       "\n",
       "[10782 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = balanced_df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45437536-4e4f-4900-bf04-cbb27c7d830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=list(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9107ff1-a578-4db9-9faa-a8222830f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=list(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13875550-8a6c-4dc4-94d7-240f4e599707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dc7c673-cbe7-4fb3-a0a8-d1680fc2478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=250,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34b4f4d-67f8-469e-aab9-be771c619665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x000001DE0FB89C60> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000001DE0FB89C60> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1079/1079 [==============================] - 6920s 6s/step - loss: 0.4073 - accuracy: 0.8172 - val_loss: 0.3057 - val_accuracy: 0.8809\n",
      "Epoch 2/2\n",
      "1079/1079 [==============================] - 6906s 6s/step - loss: 0.2386 - accuracy: 0.9061 - val_loss: 0.3078 - val_accuracy: 0.8734\n",
      "135/135 [==============================] - 512s 4s/step - loss: 0.3078 - accuracy: 0.8734\n",
      "Test loss, Test acc: [0.30775538086891174, 0.8734353184700012]\n",
      "135/135 [==============================] - 515s 4s/step\n",
      "Predictions: [[-2.3106117   2.363161  ]\n",
      " [ 2.2845702  -2.1746745 ]\n",
      " [ 0.64116704 -0.4973068 ]\n",
      " [-1.8871561   1.9206743 ]\n",
      " [-2.6253      2.6676183 ]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(y_train)))\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).shuffle(len(y_train)).batch(8)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(16)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_history = model.fit(train_dataset, validation_data=test_dataset, epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_dataset)\n",
    "print(\"Test loss, Test acc:\", results)\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(test_dataset)\n",
    "print(\"Predictions:\", predictions['logits'][:5])  # Display first 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e8195f-8e01-4b9e-84d8-3112749bad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gpava\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\save_impl.py:68: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE0A5CA690>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19923F90>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE198CFC10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19922750>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE1986A750>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE199206D0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: Distil_bert-finetune\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Distil_bert-finetune\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Distil_bert-finetune')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895317fd-f181-4d59-a210-707f7ce87e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE0A5CA690>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE0A5CA690>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19923F90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19923F90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE198CFC10>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE198CFC10>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19922750>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE19922750>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE1986A750>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE1986A750>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE199206D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x000001DE199206D0>, because it is not built.\n"
     ]
    }
   ],
   "source": [
    "model.save('distil-bert-tf', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bb19e-2fe9-4566-afcf-895374d62a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
